# config/config.yaml


# Mode d’exécution : "scrap", "crawl", "scrap+crawl"
mode: "crawl"
max_depth: 1  # pour le crawl


# Mode parallèle
parallel: true
max_workers: 5  # nombre maximal de threads simultanés


# Liste d’URLs à traiter
urls:
 - "https://example.com"
  
# Paramètres réseau
timeout: 10
retries: 3
delay: 0.5

# Paramètres HTTP
headers:
  User-Agent: "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36"
  Accept: "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8"
  Accept-Language: "fr-FR,fr;q=0.9,en-US;q=0.8,en;q=0.7"
  Accept-Encoding: "gzip, deflate"
  Connection: "keep-alive"
  Upgrade-Insecure-Requests: "1"
  Referer: "https://www.google.com/"


# Export
export_prefix: "results"

# Journalisation
log_level: "INFO"
log_format: "[%(asctime)s] [%(levelname)s] %(message)s"
